{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9730256b-943b-4b31-ab45-fa9124a4872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import fft\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "027026bf-802f-4b99-8cbe-f57d0eee7c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "2.6.0+cu118\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aa1d745-50f8-47ac-86d1-fa860d2dbc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../codes.csv') # reads in codes csv as a mutable df :-) contains message, ecl, code, dft cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b8e9152-438f-43f3-8598-7f7f94a9e714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse a colomn with a 2d array of each qr code\n",
    "df['code_array'] = df['code'].apply(lambda x: np.array([int(digit) for digit in x]).reshape(29, 29))\n",
    "df['code_oned'] = df['code'].apply(lambda x: np.array([int(digit) for digit in x])) # adds a code_oned column to our df which parses our entries from strs to ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6465f79-0683-4be2-a3d2-90ee77944c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays = np.stack(df['code_array'].values)\n",
    "average_array = np.mean(arrays, axis=0)\n",
    "\n",
    "fixedIndx = []\n",
    "for i in range(29):\n",
    "    for j in range(29):\n",
    "        if average_array[i][j] == 1:\n",
    "            fixedIndx.append((i, j))\n",
    "\n",
    "fixedIndxAll = []\n",
    "for i in range(29):\n",
    "    for j in range(29):\n",
    "        if average_array[i][j] == 1 or average_array[i][j] == 0:\n",
    "            fixedIndxAll.append((i, j))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69c7077-031c-4eda-9589-c0f65af7eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonFixed = []\n",
    "for i in range(29):\n",
    "    for j in range(29):\n",
    "        if (i,j) not in fixedIndxAll:\n",
    "            nonFixed.append((i,j))\n",
    "nonFixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89919c27-0053-4f23-a8ff-6222f7b6674b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2047989b6e0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fixedQR = np.zeros((29,29))\n",
    "for i,j in fixedIndx:\n",
    "    fixedQR[i][j] = 1\n",
    "fixedQR\n",
    "\n",
    "plt.imshow(fixedQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0208db52-62b3-4f3a-82d4-b180863dbe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabs a centered subarray of a given array with dimensions (2L+1) by (2L+1)\n",
    "def get_centered_subarray(array, L):\n",
    "    # old way -- \n",
    "    # L = (L * 2) + 1\n",
    "    # start = center - (L // 2)\n",
    "    # end = center + (L + 1) // 2\n",
    "    n = array.shape[0]\n",
    "    center = n // 2\n",
    "    start = center - L\n",
    "    end = center + 1 + L # start count after our center square\n",
    "    # Adjust indices to stay within array bounds\n",
    "    start = max(0, start)\n",
    "    end = min(n, end)\n",
    "    return array[start:end, start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d7a3d83-675f-4c1a-b66f-a7cbab51e23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to split DFT into real and imaginary parts and reshape them\n",
    "def split_dft(dft_str):\n",
    "    complex_numbers = [complex(c.strip()) for c in dft_str.strip('()').split(')(')]\n",
    "    real_part = np.array([c.real for c in complex_numbers])\n",
    "    imag_part = np.array([c.imag for c in complex_numbers])\n",
    "    return real_part, imag_part\n",
    "\n",
    "# apply the function to each row\n",
    "df['dft_real'], df['dft_imag'] = zip(*df['dft'].apply(split_dft))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b410b0d-0ec6-40ef-947c-e802caf37cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to take dft string into complex tuple stored as (a, ib)\n",
    "def combine_dft_full(dft_str):\n",
    "    complex_numbers = [complex(c.strip()) for c in dft_str.strip('()').split(')(')]\n",
    "    real_part = np.array([c.real for c in complex_numbers])\n",
    "    # real_part = zero_pad_right(0.5, real_part)\n",
    "    imag_part = np.array([c.imag for c in complex_numbers])\n",
    "    # imag_part = zero_pad_right(0.5, imag_part)\n",
    "    return np.append(real_part, imag_part)\n",
    "\n",
    "df['dft_stacked'] = df['dft'].apply(combine_dft_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b4efa18-9a50-4789-a5eb-38c087496726",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_fft = fft.fft2(fixedQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8703ac2-fc06-440f-9011-62c95e329370",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = fixed_fft.copy().flatten() # flatten fixed \n",
    "\n",
    "# subtracts flattened fixed qr fft from flattened qr fft\n",
    "def adjust(stacked):\n",
    "    copy = stacked.copy()\n",
    "    for i in range(841): \n",
    "        copy[i] -= flat[i].real\n",
    "        copy[i+841] -= flat[i].imag # imaginary numbers shifted by 841 (total len)\n",
    "    return copy\n",
    "\n",
    "df['adjusted'] = df['dft_stacked'].apply(adjust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4251b97-2d6b-4d29-b691-1d8f232dbd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_y(flat):\n",
    "    new = np.array([flat[row * 29 + column] for row, column in nonFixed])\n",
    "    return new\n",
    "\n",
    "df['y_adjusted'] = df['code_oned'].apply(adjust_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40f2d2c2-0b19-4bd0-9fab-84c519d53cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (X) - DFT components\n",
    "X = np.stack(df['adjusted'].values)  # Shape: (num_samples, 1682) - (flat) total input data\n",
    "\n",
    "# Labels (y) - QR codes\n",
    "y = np.stack(df['y_adjusted'].values)  # Shape: (num_samples, 841) - (flat)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # decide on a rseed of 42 and a set set of 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a447cf0-2a18-4b8d-b506-18ba447c7287",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Model, self).__init__()\n",
    "        input_size = ((((L*2)+1)**2)*2)  # Matches the tf.keras InputLayer\n",
    "        output_size = 592  # Matches the final Dense(592) layer\n",
    "        \n",
    "        self.fc = nn.Linear(input_size, output_size)  # Single-layer\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22a8893d-f7cc-4866-aeb5-628a3a99b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, num_samples=5):\n",
    "    \"\"\"\n",
    "    evaluates the trained model on the test set and displays a few samples with their predictions.\n",
    "\n",
    "    parameters:\n",
    "    - model: The trained TensorFlow/Keras model.\n",
    "    - X_test: The input features for the test set (shape: (num_samples, 1682)).\n",
    "    - y_test: The true labels for the test set (shape: (num_samples, 841)).\n",
    "    - num_samples: The number of test samples to display.\n",
    "\n",
    "    returns:\n",
    "    - accuracy: The overall accuracy of the model on the test set.\n",
    "    \"\"\"\n",
    "    # make predictions on the test set\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # binarize the predictions (0 or 1)\n",
    "    predictions_binarized = (predictions > 0.5).astype(np.int32)\n",
    "    \n",
    "    # calculate accuracy\n",
    "    correct_predictions = np.sum(predictions_binarized == y_test)\n",
    "    total_elements = y_test.size\n",
    "    accuracy = correct_predictions / total_elements\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "        \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f58ebb7-29c8-4ea2-b8eb-70985fc4faaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dft_rolled'] = df['adjusted'].apply(lambda x: np.roll(x.reshape(29,29,2), shift=(14, 14), axis=(0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2e424c9-c3da-40bd-91d6-a3199a28951c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Convert to PyTorch tensors\u001b[39;00m\n\u001b[32m     57\u001b[39m X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m y_train_tensor = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n\u001b[32m     60\u001b[39m train_loader = DataLoader(train_dataset, batch_size=\u001b[32m32\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "# Define Model Class\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, L):\n",
    "        super(Model, self).__init__()\n",
    "        input_size = ((((L*2)+1)**2)*2)  # Adjusts input size for L\n",
    "        output_size = 592  # Matches TensorFlow model\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)  # No activation for regression\n",
    "\n",
    "# Training Loop\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=25):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        predictions = model(X_test)\n",
    "        accuracy = (predictions.argmax(dim=1) == y_test.argmax(dim=1)).float().mean().item()\n",
    "    return accuracy\n",
    "\n",
    "# Device Setup (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Main Loop Over L Values\n",
    "L = 10\n",
    "stat = []\n",
    "for L in range(10, 0, -1):\n",
    "    # Extract L features\n",
    "    df['L'] = df['dft_rolled'].apply(lambda x: get_centered_subarray(x, L).flatten())\n",
    "\n",
    "    # Convert features to numpy arrays\n",
    "    X = np.stack(df['L'].values)  # Shape: (num_samples, ((L*2)+1)*2)\n",
    "    y = np.stack(df['y_adjusted'].values)  # Assuming `df['target']` contains labels\n",
    "\n",
    "    # Split dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Initialize Model, Optimizer, and Loss Function\n",
    "    model = Model(L).to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()  # Assuming a regression task\n",
    "\n",
    "    # Train Model\n",
    "    train_model(model, train_loader, criterion, optimizer, epochs=25)\n",
    "\n",
    "    # Evaluate Model\n",
    "    accuracy = evaluate_model(model, X_test, y_test)\n",
    "    stat.append((L, accuracy))\n",
    "\n",
    "    print(stat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aafc53-9c8c-47e9-863e-fdb22f5cd805",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "ax.set_xlabel('Epochs', fontsize=14)\n",
    "ax.set_ylabel('Pixel Accuracy (%)', fontsize=14)\n",
    "ax.set_title('L10 70k samples: Acc vs epoch')\n",
    "ax.plot([st[0] for st in stat], [st[1] for st in stat]) # plots accuracy vs L -- we can add labels later!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
