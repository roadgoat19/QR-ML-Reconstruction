{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da55bb62-cc1c-4356-9a1a-bfa78ec13505",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, glob\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75e44a91-92e6-46cc-8ebe-ef9d21f150d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('codes.csv') # read in qr code data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae778081-b542-4219-8da5-3333c186e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse a column with a 2d array of each qr code and a one d array of each qr code\n",
    "df['code_array'] = df['code'].apply(lambda x: np.array([int(digit) for digit in x]).reshape(29, 29))\n",
    "df['code_oned'] = df['code'].apply(lambda x: np.array([int(digit) for digit in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a7ebdaa-a15e-4bf9-b0d1-bd739bee6dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in an array and L parameter and returns centered submatrix of length 2L+1\n",
    "def get_centered_subarray(array, L):\n",
    "    L = (L * 2) + 1 # length\n",
    "    n = array.shape[0]\n",
    "    center = n // 2\n",
    "    start = center - (L // 2)\n",
    "    end = center + (L + 1) // 2\n",
    "    # Adjust indices to stay within array bounds (allows for wrapping)\n",
    "    start = max(0, start)\n",
    "    end = min(n, end)\n",
    "    return array[start:end, start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4216e51-0a4b-49a2-97bf-aa37278f8c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split DFT into real and imaginary parts and reshape them\n",
    "def split_dft(dft_str):\n",
    "    complex_numbers = [complex(c.strip()) for c in dft_str.strip('()').split(')(')]\n",
    "    real_part = np.array([c.real for c in complex_numbers])\n",
    "    imag_part = np.array([c.imag for c in complex_numbers])\n",
    "    return real_part, imag_part\n",
    "\n",
    "# Apply the function to each row\n",
    "df['dft_real'], df['dft_imag'] = zip(*df['dft'].apply(split_dft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97831ab2-3513-4913-9fc6-d57f02a6d250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (deprecated function) zero pads from the right\n",
    "def zero_pad_right(per, arr):\n",
    "    index = int(per * len(arr))\n",
    "    for i in range(index, len(arr)):\n",
    "        arr[i] = 0\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c54dbf5-ca10-4dc9-a2fd-56e5ec9a8a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (deprecated function) parse dft's data to 1d array of length 29**2*2\n",
    "\n",
    "def combine_dft_full(dft_str):\n",
    "    complex_numbers = [complex(c.strip()) for c in dft_str.strip('()').split(')(')]\n",
    "    real_part = np.array([c.real for c in complex_numbers])\n",
    "    # real_part = zero_pad_right(0.5, real_part)\n",
    "    imag_part = np.array([c.imag for c in complex_numbers])\n",
    "    # imag_part = zero_pad_right(0.5, imag_part)\n",
    "    return np.append(real_part, imag_part)\n",
    "\n",
    "df['dft_stacked'] = df['dft'].apply(combine_dft_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b657857a-e62b-43e8-a778-e32f9c0a0c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.27000000e+02, -2.81751760e+00,  2.43744204e-01, ...,\n",
       "       -5.08998548e+00,  1.27562664e+01,  8.68168459e+00])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['dft_stacked'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7108b75f-2ef5-4cc3-9e66-c4b088209485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The X here is deprecated becasue we first need to transform into L matrix\n",
    "\n",
    "# Features (X) - DFT components\n",
    "# X = np.stack(df['dft_stacked'].values)  # Shape: (num_samples, 1682)\n",
    "\n",
    "# Labels (y) - QR codes\n",
    "y = np.stack(df['code_array'].values)  # Shape: (num_samples, 29,29)\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebc9a7a7-f184-4917-b4f1-4de6b2b8d6f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_tr, X_te \u001b[38;5;241m=\u001b[39m split_data(\u001b[43mdfts\u001b[49m, \u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m12345\u001b[39m)\n\u001b[0;32m      2\u001b[0m y_tr, y_te \u001b[38;5;241m=\u001b[39m split_data(codes, \u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m12345\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dfts' is not defined"
     ]
    }
   ],
   "source": [
    "# X_tr, X_te = split_data(df[']dfts, 0.8, 12345)\n",
    "# y_tr, y_te = split_data(codes, 0.8, 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2e11ffd-75f2-4ad1-9c7e-0a6706122812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 27, 27, 32)        608       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 576)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               295424    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 53824)             27611712  \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 29, 29, 64)        0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 29, 29, 64)       36928     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 29, 29, 32)       18464     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 29, 29, 1)         289       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,018,849\n",
      "Trainable params: 28,018,849\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# (deprecated) overcomplicated model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Defining CNN model\n",
    "multilayered_model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(29, 29, 2)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(29 * 29 * 64, activation='relu'),  # Adjusting to match the target shape\n",
    "    layers.Reshape((29, 29, 64)),\n",
    "    layers.Conv2DTranspose(64, (3, 3), strides=(1, 1), padding='same', activation='relu'),\n",
    "    layers.Conv2DTranspose(32, (3, 3), strides=(1, 1), padding='same', activation='relu'),\n",
    "    layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')  # Output should match the shape of QR code (29x29)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "multilayered_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "multilayered_model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eebcd56-3b99-4099-b5e1-6568f02f975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to initialize model for a given L size input\n",
    "\n",
    "def init_model(L):\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # Add the input layer (implicitly defined by the first layer) size of the L matrix times 2 to include both real and imaginary components\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=((((L*2)+1)**2)*2,)))\n",
    "    \n",
    "    # Add the output layer with Y units (no activation for regression, 'softmax' or 'sigmoid' for classification)\n",
    "    model.add(tf.keras.layers.Dense(841))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "    # Compile the model (use appropriate loss and metrics for your task)\n",
    "    model.compile(optimizer=optimizer, loss='mse')  # or 'categorical_crossentropy' for classification\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff052af9-fc84-4c9a-a059-bc792771e541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to initialize model for a given L size input with convo layer\n",
    "\n",
    "def init_model_convo(L):\n",
    "    L = (((L*2)+1)**2)*2\n",
    "    \n",
    "    # Initialize the Sequential model\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # Input layer: Accepts (L, L, 2) where L < 29\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=(None, None, 2)))  # None allows for variable L\n",
    "    \n",
    "    # Convolutional Layer\n",
    "    model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "    \n",
    "    # Resizing Layer to upscale to 29x29\n",
    "    model.add(tf.keras.layers.Resizing(height=29, width=29, interpolation='bilinear'))\n",
    "    \n",
    "    # Output Layer\n",
    "    model.add(tf.keras.layers.Conv2D(filters=1, kernel_size=(3, 3), activation='sigmoid', padding='same'))\n",
    "    \n",
    "    # Display the model summary\n",
    "    model.summary()\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9acf751f-7eb0-4bac-9b8a-68985830b3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential([Dense(29*29*2, input_dim=29*29*2, use_bias=False)])\n",
    "# model.compile(loss='mean_squared_error', optimizer='adam', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de2cd045-513f-4b7e-bfdd-0bf1d56bb940",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (1682,) into shape (29,29,2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mset\u001b[39m \u001b[38;5;129;01min\u001b[39;00m [X_train, X_test, y_train, y_test]:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m)):\n\u001b[1;32m----> 3\u001b[0m         \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m[i]\u001b[38;5;241m.\u001b[39mflatten()\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (1682,) into shape (29,29,2)"
     ]
    }
   ],
   "source": [
    "# flatten data (DEPRECATED)\n",
    "for set in [X_train, X_test, y_train, y_test]:\n",
    "    for i in range(len(set)):\n",
    "        set[i] = set[i].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c1fd4ff-b94c-452d-bb36-b61f41e86b5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(\u001b[43mmodel\u001b[49m, X_test[:\u001b[38;5;241m999\u001b[39m], y_test[:\u001b[38;5;241m999\u001b[39m], num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, num_samples=5):\n",
    "    \"\"\"\n",
    "    Evaluates the trained model on the test set and displays a few samples with their predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained TensorFlow/Keras model.\n",
    "    - X_test: The input features for the test set (shape: (num_samples, 1682)).\n",
    "    - y_test: The true labels for the test set (shape: (num_samples, 841)).\n",
    "    - num_samples: The number of test samples to display.\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: The overall accuracy of the model on the test set.\n",
    "    \"\"\"\n",
    "    # Make predictions on the test set\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Binarize the predictions (0 or 1)\n",
    "    predictions_binarized = (predictions > 0.5).astype(np.int32)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    correct_predictions = np.sum(predictions_binarized == y_test)\n",
    "    total_elements = y_test.size\n",
    "    accuracy = correct_predictions / total_elements\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Display some predictions\n",
    "    for i in range(num_samples):\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        # Original QR code\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title(\"Original QR Code\")\n",
    "        plt.imshow(y_test[i].reshape(29, 29), cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Predicted QR code\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title(\"Predicted QR Code\")\n",
    "        plt.imshow(predictions_binarized[i].reshape(29, 29), cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Difference (Error)\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(\"Difference (Error)\")\n",
    "        plt.imshow(y_test[i].reshape(29, 29) - predictions_binarized[i].reshape(29, 29), cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    return accuracy\n",
    "\n",
    "# Example usage:\n",
    "accuracy = evaluate_model(model, X_test[:999], y_test[:999], num_samples=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cd05ca9-d750-418f-b378-9c5c9a527239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dft(dft_str):\n",
    "    complex_numbers = [complex(c.strip()) for c in dft_str.strip('()').split(')(')]\n",
    "    real_part = np.array([c.real for c in complex_numbers])\n",
    "    real_part = zero_pad_right(per, real_part)\n",
    "    imag_part = np.array([c.imag for c in complex_numbers])\n",
    "    imag_part = zero_pad_right(per, imag_part)\n",
    "    return np.append(real_part, imag_part)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "172fcef6-30f2-435f-b6de-74d15171da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take in full size 1d array,reshape it to original 29 by 29, and shift it so center is in right place\n",
    "df['dft_imag_rolled'] = df['dft_imag'].apply(lambda x: np.roll(x.reshape(29,29), shift=(14, 14), axis=(0, 1)))\n",
    "df['dft_real_rolled'] = df['dft_real'].apply(lambda x: np.roll(x.reshape(29,29), shift=(14, 14), axis=(0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b896cfe-f32d-4616-83b8-74721e8fd1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 29)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['dft_imag_rolled'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79d05682-aa27-44d4-af28-ad5f23856d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(441,)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'InputLayer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#concatenate imag and real 1d arrays\u001b[39;00m\n\u001b[0;32m     12\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: np\u001b[38;5;241m.\u001b[39mconcatenate((row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL_real\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL_imag\u001b[39m\u001b[38;5;124m'\u001b[39m])), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43minit_model_convo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Features (X) - DFT components\u001b[39;00m\n\u001b[0;32m     15\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)  \u001b[38;5;66;03m# Shape: (num_samples, ((L*2)+1)*2)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m, in \u001b[0;36minit_model_convo\u001b[1;34m(L)\u001b[0m\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Input layer: Accepts (L, L, 2) where L < 29\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m model\u001b[38;5;241m.\u001b[39madd(\u001b[43mInputLayer\u001b[49m(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m2\u001b[39m)))  \u001b[38;5;66;03m# None allows for variable L\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Convolutional Layer\u001b[39;00m\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Conv2D(filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'InputLayer' is not defined"
     ]
    }
   ],
   "source": [
    "# (depreacted loop from 10/2/24) loop to train model for different L sizes. use other loop for convo.\n",
    "\n",
    "# stat stores L size with accuracy for plotting after\n",
    "# stat = []\n",
    "# for L in range(10,0,-1):\n",
    "#     # get L matrices flattened\n",
    "#     df['L_imag'] = df['dft_imag_rolled'].apply(lambda x: get_centered_subarray(x, L).flatten())\n",
    "#     df['L_real']= df['dft_imag_rolled'].apply(lambda x: get_centered_subarray(x, L).flatten())\n",
    "#     # diagnostic print\n",
    "#     print(df['L_imag'][0].shape)\n",
    "#     # concatenate imag and real 1d arrays\n",
    "#     df['L'] = df.apply(lambda row: np.concatenate((row['L_real'], row['L_imag'])), axis=1)\n",
    "#     model = init_model_convo(L)\n",
    "#     # Features (X) - DFT components\n",
    "#     X = np.stack(df['L'].values)  # Shape: (num_samples, ((L*2)+1)*2)\n",
    "#     print(X.shape)\n",
    "#     # Split the data into training and testing sets\n",
    "#     # y sets should be initialized outside loop since they don't change\n",
    "#     from sklearn.model_selection import train_test_split\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#     # Fit the model\n",
    "#     model.fit(X_train, y_train, epochs=100, validation_data=[X_test[1000:], y_test[1000:]])\n",
    "\n",
    "#     # eval model\n",
    "#     accuracy = evaluate_model(model, X_test[:999], y_test[:999], num_samples=1)\n",
    "#     stat.append((L, accuracy))\n",
    "\n",
    "# print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a15b28-ea98-4b6e-bb77-ea837b7c0194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop to TRAIN model over different L values (use other loop for CNN)\n",
    "from sklearn.model_selection import train_test_split\n",
    "# stat stores L size with accuracy for plotting after\n",
    "stat = []\n",
    "for L in range(10,0,-1): # iterate through 10 - 1\n",
    "    # get L matrices flattened\n",
    "    df['L_imag'] = df['dft_imag_rolled'].apply(lambda x: get_centered_subarray(x, L).flatten())\n",
    "    df['L_real']= df['dft_imag_rolled'].apply(lambda x: get_centered_subarray(x, L).flatten())\n",
    "    # diagnostic print\n",
    "    print(df['L_imag'][0].shape)\n",
    "    # concatenate imag and real 1d arrays\n",
    "    df['L'] = df.apply(lambda row: np.concatenate((row['L_real'], row['L_imag'])), axis=1)\n",
    "    model = init_model(L)\n",
    "    # Features (X) - DFT components\n",
    "    X = np.stack(df['L'].values)  # Shape: (num_samples, ((L*2)+1)*2)\n",
    "    print(X.shape)\n",
    "    # Split the data into training and testing sets\n",
    "    # y sets should be initialized outside loop since they don't change\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train, epochs=100, validation_data=[X_test[1000:], y_test[1000:]])\n",
    "\n",
    "    # eval model\n",
    "    accuracy = evaluate_model(model, X_test[:999], y_test[:999], num_samples=1)\n",
    "    stat.append((L, accuracy))\n",
    "\n",
    "print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e734fa54-b365-4090-85e4-9e0a9f4caab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to combine real and imaginary parts\n",
    "def combine_real_imag(row):\n",
    "    # Stack along the last axis to create a new dimension for channels\n",
    "    combined = np.stack((row['L_real'], row['L_imag']), axis=-1)\n",
    "    return combined  # Shape: (L, L, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13bb79b3-3cea-4f36-9b5c-d540c74b44b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, num_samples=999):\n",
    "    \"\"\"\n",
    "    Evaluate the model's pixel-wise accuracy on a subset of the test data.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained Keras model.\n",
    "    - X: Test input data, shape (num_samples_total, L, L, 2).\n",
    "    - y: True QR codes, shape (num_samples_total, 29, 29, 1).\n",
    "    - num_samples: Number of samples to evaluate.\n",
    "    \n",
    "    Returns:\n",
    "    - average_accuracy: Average pixel-wise accuracy over the selected samples.\n",
    "    \"\"\"\n",
    "    # Ensure num_samples does not exceed the available samples\n",
    "    num_samples = min(num_samples, X.shape[0])\n",
    "    \n",
    "    # Predict the QR codes\n",
    "    y_pred = model.predict(X[:num_samples])\n",
    "    \n",
    "    # Check if the shapes match\n",
    "    if y_pred.shape != y[:num_samples].shape:\n",
    "        # If y_pred is (num_samples, 29, 29, 1) and y is (num_samples, 29, 29, 1), it's okay\n",
    "        # Otherwise, attempt to reshape if possible\n",
    "        try:\n",
    "            y_pred = y_pred.reshape(y[:num_samples].shape)\n",
    "        except:\n",
    "            raise ValueError(f\"Shape mismatch: y_pred shape {y_pred.shape} vs y shape {y[:num_samples].shape}\")\n",
    "    \n",
    "    # Apply a threshold to convert predictions to binary\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate pixel-wise accuracy\n",
    "    correct_pixels = (y_pred_binary == y[:num_samples]).astype(int)\n",
    "    pixel_accuracy = correct_pixels.mean()\n",
    "    \n",
    "    # maybe try removing fixed structure to show message encoded pixels accuracy?\n",
    "    \n",
    "    return pixel_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6cb4925-8c10-434d-8e63-ce02ce7f41ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 21, 2)\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_20 (Conv2D)          (None, None, None, 16)    304       \n",
      "                                                                 \n",
      " resizing_10 (Resizing)      (None, 29, 29, 16)        0         \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 29, 29, 1)         145       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 449\n",
      "Trainable params: 449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(9999, 21, 21, 2)\n",
      "Epoch 1/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.3516 - val_loss: 0.3102\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2938 - val_loss: 0.2823\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2750 - val_loss: 0.2696\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2655 - val_loss: 0.2624\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2599 - val_loss: 0.2579\n",
      "Epoch 6/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2564 - val_loss: 0.2552\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2543 - val_loss: 0.2534\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2528 - val_loss: 0.2523\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2517 - val_loss: 0.2512\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2508 - val_loss: 0.2505\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2502 - val_loss: 0.2499\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2497 - val_loss: 0.2495\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2492 - val_loss: 0.2490\n",
      "Epoch 14/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2489 - val_loss: 0.2487\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2485 - val_loss: 0.2484\n",
      "Epoch 16/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2483 - val_loss: 0.2481\n",
      "Epoch 17/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2480 - val_loss: 0.2479\n",
      "Epoch 18/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2478 - val_loss: 0.2477\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2476 - val_loss: 0.2475\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2474 - val_loss: 0.2474\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2472 - val_loss: 0.2471\n",
      "Epoch 22/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2470 - val_loss: 0.2470\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2469 - val_loss: 0.2468\n",
      "Epoch 24/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2468 - val_loss: 0.2467\n",
      "Epoch 25/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2466 - val_loss: 0.2466\n",
      "Epoch 26/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2465 - val_loss: 0.2465\n",
      "Epoch 27/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2464 - val_loss: 0.2463\n",
      "Epoch 28/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2463 - val_loss: 0.2462\n",
      "Epoch 29/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2462 - val_loss: 0.2462\n",
      "Epoch 30/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2461 - val_loss: 0.2460\n",
      "Epoch 31/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2460 - val_loss: 0.2460\n",
      "Epoch 32/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2459 - val_loss: 0.2459\n",
      "Epoch 33/50\n",
      "250/250 [==============================] - 3s 14ms/step - loss: 0.2458 - val_loss: 0.2458\n",
      "Epoch 34/50\n",
      "250/250 [==============================] - 3s 13ms/step - loss: 0.2458 - val_loss: 0.2458\n",
      "Epoch 35/50\n",
      "250/250 [==============================] - 3s 13ms/step - loss: 0.2457 - val_loss: 0.2457\n",
      "Epoch 36/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.2456 - val_loss: 0.2456\n",
      "Epoch 37/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2456 - val_loss: 0.2456\n",
      "Epoch 38/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2455 - val_loss: 0.2455\n",
      "Epoch 39/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2454 - val_loss: 0.2455\n",
      "Epoch 40/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2454 - val_loss: 0.2454\n",
      "Epoch 41/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2453 - val_loss: 0.2453\n",
      "Epoch 42/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.2453 - val_loss: 0.2453\n",
      "Epoch 43/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2452 - val_loss: 0.2453\n",
      "Epoch 44/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2452 - val_loss: 0.2452\n",
      "Epoch 45/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2451 - val_loss: 0.2452\n",
      "Epoch 46/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2451 - val_loss: 0.2451\n",
      "Epoch 47/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.2451 - val_loss: 0.2451\n",
      "Epoch 48/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2450 - val_loss: 0.2451\n",
      "Epoch 49/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.2450 - val_loss: 0.2450\n",
      "Epoch 50/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.2449 - val_loss: 0.2450\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001DE123E21F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "[(10, 0.5564803804994055)]\n",
      "(19, 19, 2)\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_22 (Conv2D)          (None, None, None, 16)    304       \n",
      "                                                                 \n",
      " resizing_11 (Resizing)      (None, 29, 29, 16)        0         \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 29, 29, 1)         145       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 449\n",
      "Trainable params: 449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(9999, 19, 19, 2)\n",
      "Epoch 1/50\n",
      "250/250 [==============================] - 4s 13ms/step - loss: 0.3348 - val_loss: 0.3069\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 3s 13ms/step - loss: 0.2929 - val_loss: 0.2816\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2748 - val_loss: 0.2687\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 0.2650 - val_loss: 0.2616\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2596 - val_loss: 0.2576\n",
      "Epoch 6/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2562 - val_loss: 0.2546\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 3s 14ms/step - loss: 0.2536 - val_loss: 0.2526\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 3s 14ms/step - loss: 0.2520 - val_loss: 0.2514\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 3s 13ms/step - loss: 0.2510 - val_loss: 0.2506\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 0.2503 - val_loss: 0.2499\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 0.2497 - val_loss: 0.2494\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 0.2493 - val_loss: 0.2490\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 0.2489 - val_loss: 0.2487\n",
      "Epoch 14/50\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 0.2486 - val_loss: 0.2484\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2483 - val_loss: 0.2481\n",
      "Epoch 16/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2481 - val_loss: 0.2480\n",
      "Epoch 17/50\n",
      "250/250 [==============================] - 3s 13ms/step - loss: 0.2479 - val_loss: 0.2478\n",
      "Epoch 18/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2477 - val_loss: 0.2476\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - 3s 13ms/step - loss: 0.2476 - val_loss: 0.2474\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 0.2474 - val_loss: 0.2473\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.2473 - val_loss: 0.2472\n",
      "Epoch 22/50\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 0.2472 - val_loss: 0.2471\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.2471 - val_loss: 0.2470\n",
      "Epoch 24/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2470 - val_loss: 0.2469\n",
      "Epoch 25/50\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 0.2469 - val_loss: 0.2468\n",
      "Epoch 26/50\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 0.2468 - val_loss: 0.2467\n",
      "Epoch 27/50\n",
      "250/250 [==============================] - 5s 19ms/step - loss: 0.2467 - val_loss: 0.2467\n",
      "Epoch 28/50\n",
      "250/250 [==============================] - 5s 22ms/step - loss: 0.2466 - val_loss: 0.2466\n",
      "Epoch 29/50\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 0.2466 - val_loss: 0.2465\n",
      "Epoch 30/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2465 - val_loss: 0.2464\n",
      "Epoch 31/50\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 0.2464 - val_loss: 0.2464\n",
      "Epoch 32/50\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 0.2464 - val_loss: 0.2463\n",
      "Epoch 33/50\n",
      "250/250 [==============================] - 4s 18ms/step - loss: 0.2463 - val_loss: 0.2462\n",
      "Epoch 34/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.2462 - val_loss: 0.2462\n",
      "Epoch 35/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2462 - val_loss: 0.2461\n",
      "Epoch 36/50\n",
      "250/250 [==============================] - 4s 18ms/step - loss: 0.2461 - val_loss: 0.2460\n",
      "Epoch 37/50\n",
      "250/250 [==============================] - 5s 18ms/step - loss: 0.2461 - val_loss: 0.2460\n",
      "Epoch 38/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2460 - val_loss: 0.2460\n",
      "Epoch 39/50\n",
      "250/250 [==============================] - 3s 13ms/step - loss: 0.2459 - val_loss: 0.2459\n",
      "Epoch 40/50\n",
      "250/250 [==============================] - 5s 19ms/step - loss: 0.2459 - val_loss: 0.2458\n",
      "Epoch 41/50\n",
      "250/250 [==============================] - 5s 21ms/step - loss: 0.2458 - val_loss: 0.2458\n",
      "Epoch 42/50\n",
      "250/250 [==============================] - 3s 13ms/step - loss: 0.2458 - val_loss: 0.2457\n",
      "Epoch 43/50\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.2457 - val_loss: 0.2457\n",
      "Epoch 44/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.2457 - val_loss: 0.2456\n",
      "Epoch 45/50\n",
      "250/250 [==============================] - 3s 13ms/step - loss: 0.2456 - val_loss: 0.2456\n",
      "Epoch 46/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.2456 - val_loss: 0.2455\n",
      "Epoch 47/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.2456 - val_loss: 0.2455\n",
      "Epoch 48/50\n",
      "250/250 [==============================] - 3s 13ms/step - loss: 0.2455 - val_loss: 0.2454\n",
      "Epoch 49/50\n",
      "250/250 [==============================] - 3s 13ms/step - loss: 0.2455 - val_loss: 0.2454\n",
      "Epoch 50/50\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 0.2454 - val_loss: 0.2454\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "[(10, 0.5564803804994055), (9, 0.5457788347205708)]\n",
      "(17, 17, 2)\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_24 (Conv2D)          (None, None, None, 16)    304       \n",
      "                                                                 \n",
      " resizing_12 (Resizing)      (None, 29, 29, 16)        0         \n",
      "                                                                 \n",
      " conv2d_25 (Conv2D)          (None, 29, 29, 1)         145       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 449\n",
      "Trainable params: 449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(9999, 17, 17, 2)\n",
      "Epoch 1/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.3304 - val_loss: 0.3026\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 0.2899 - val_loss: 0.2797\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 0.2737 - val_loss: 0.2687\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 3s 13ms/step - loss: 0.2654 - val_loss: 0.2626\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 0.2606 - val_loss: 0.2588\n",
      "Epoch 6/50\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 0.2575 - val_loss: 0.2563\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2553 - val_loss: 0.2543\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2536 - val_loss: 0.2530\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 3s 14ms/step - loss: 0.2524 - val_loss: 0.2520\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.2516 - val_loss: 0.2514\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 3s 14ms/step - loss: 0.2509 - val_loss: 0.2507\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.2504 - val_loss: 0.2502\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 3s 14ms/step - loss: 0.2500 - val_loss: 0.2498\n",
      "Epoch 14/50\n",
      "250/250 [==============================] - 3s 13ms/step - loss: 0.2496 - val_loss: 0.2494\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - 3s 14ms/step - loss: 0.2493 - val_loss: 0.2491\n",
      "Epoch 16/50\n",
      "250/250 [==============================] - 3s 14ms/step - loss: 0.2490 - val_loss: 0.2491\n",
      "Epoch 17/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2488 - val_loss: 0.2487\n",
      "Epoch 18/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2486 - val_loss: 0.2485\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - 3s 13ms/step - loss: 0.2484 - val_loss: 0.2483\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - 3s 14ms/step - loss: 0.2483 - val_loss: 0.2481\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.2481 - val_loss: 0.2480\n",
      "Epoch 22/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2480 - val_loss: 0.2480\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 0.2478 - val_loss: 0.2478\n",
      "Epoch 24/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2477 - val_loss: 0.2476\n",
      "Epoch 25/50\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 0.2476 - val_loss: 0.2475\n",
      "Epoch 26/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2475 - val_loss: 0.2474\n",
      "Epoch 27/50\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 0.2474 - val_loss: 0.2473\n",
      "Epoch 28/50\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 0.2473 - val_loss: 0.2472\n",
      "Epoch 29/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.2472 - val_loss: 0.2471\n",
      "Epoch 30/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2471 - val_loss: 0.2470\n",
      "Epoch 31/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2471 - val_loss: 0.2470\n",
      "Epoch 32/50\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 0.2470 - val_loss: 0.2469\n",
      "Epoch 33/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2469 - val_loss: 0.2468\n",
      "Epoch 34/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2468 - val_loss: 0.2468\n",
      "Epoch 35/50\n",
      "250/250 [==============================] - 3s 13ms/step - loss: 0.2467 - val_loss: 0.2467\n",
      "Epoch 36/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2467 - val_loss: 0.2466\n",
      "Epoch 37/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2466 - val_loss: 0.2465\n",
      "Epoch 38/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2466 - val_loss: 0.2465\n",
      "Epoch 39/50\n",
      "250/250 [==============================] - 3s 13ms/step - loss: 0.2465 - val_loss: 0.2464\n",
      "Epoch 40/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2464 - val_loss: 0.2463\n",
      "Epoch 41/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2464 - val_loss: 0.2463\n",
      "Epoch 42/50\n",
      "250/250 [==============================] - 3s 14ms/step - loss: 0.2463 - val_loss: 0.2462\n",
      "Epoch 43/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.2463 - val_loss: 0.2462\n",
      "Epoch 44/50\n",
      "250/250 [==============================] - 3s 14ms/step - loss: 0.2462 - val_loss: 0.2461\n",
      "Epoch 45/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2462 - val_loss: 0.2461\n",
      "Epoch 46/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2461 - val_loss: 0.2460\n",
      "Epoch 47/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2461 - val_loss: 0.2460\n",
      "Epoch 48/50\n",
      "250/250 [==============================] - 3s 13ms/step - loss: 0.2460 - val_loss: 0.2459\n",
      "Epoch 49/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2460 - val_loss: 0.2459\n",
      "Epoch 50/50\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.2459 - val_loss: 0.2458\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "[(10, 0.5564803804994055), (9, 0.5457788347205708), (8, 0.5552913198573127)]\n"
     ]
    }
   ],
   "source": [
    "# loop to train model for different L sizes. use this loop for CNN (?)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# stat stores L size with accuracy for plotting after\n",
    "stat = []\n",
    "for L in [10, 9, 8]:\n",
    "    # get L matrices flattened\n",
    "    df['L_imag'] = df['dft_imag_rolled'].apply(lambda x: get_centered_subarray(x, L))\n",
    "    df['L_real']= df['dft_real_rolled'].apply(lambda x: get_centered_subarray(x, L))\n",
    "\n",
    "    # Apply the function to each row to create the 'L_combined' column\n",
    "    df['L'] = df.apply(combine_real_imag, axis=1)\n",
    "    # diagnostic print\n",
    "    print(df['L'][0].shape)\n",
    "    # concatenate imag and real 1d arrays\n",
    "    model = init_model_convo(L)\n",
    "    # Features (X) - DFT components\n",
    "    X = np.stack(df['L'].values)  \n",
    "    print(X.shape)\n",
    "    # Split the data into training and testing sets\n",
    "\n",
    "    # y sets should be initialized outside loop since they don't change\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train, epochs=50, validation_data=[X_test[1000:], y_test[1000:]]) # currently only doing 50 epochs for 3 values of L\n",
    "    # we should try 100-150 epochs for one value of L to start (maybe starting at 12?)\n",
    "\n",
    "    # eval model\n",
    "    accuracy = evaluate_model(model, X_test[:999], y_test[:999], num_samples=1)\n",
    "    \n",
    "    stat.append((L, accuracy)) # L parameter used and accuracy achieved\n",
    "\n",
    "    print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffd6ca29-7df5-43c0-b2ac-3dd4ac1cf9c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, 0.5481569560047562)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5a5be06d-c13b-4db7-bccf-9e8331f08f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.4927174499112668),\n",
       " (0.05, 0.7652337236166011),\n",
       " (0.1, 0.7877592217663562),\n",
       " (0.15, 0.8040549467422238),\n",
       " (0.2, 0.822725222249598),\n",
       " (0.25, 0.8430439952437574),\n",
       " (0.3, 0.87219800061655),\n",
       " (0.35, 0.8962351174003969),\n",
       " (0.4, 0.9238846456444554),\n",
       " (0.45, 0.954785939328151),\n",
       " (0.5, 0.9956436817316722)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rstat = stat[::-1]\n",
    "rstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36dc24d8-4f1f-4642-b5c1-564294819b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x157b5adf460>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([st[0] for st in stat], [st[1] for st in stat]) # plot of L vs accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61767818-a8ee-48bb-93d4-986b12c3d993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci373",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
